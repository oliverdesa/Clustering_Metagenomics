{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from functions import *\n",
    "import os\n",
    "import subprocess\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enzymes = ['dl_endopeptidase', 'ld_carboxypeptidase', \n",
    "               'ld_endopeptidase', 'amidase',\n",
    "               'dd_carboxypeptidase', 'diadenylate_cyclase',\n",
    "               'muramidase', 'glucosaminidase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/clustering/cluster_maps/dl_endopeptidases.tsv',sep='\\t')\n",
    "\n",
    "df = df.iloc[:, 1:4]\n",
    "\n",
    "df = df[pd.isna(df[df.columns[2]])]\n",
    "\n",
    "grouped = df.groupby('dl_endopeptidases-mmseqs_cluster')['dl_endopeptidases-unclustered'].apply(list).reset_index(name='unclustered_list')\n",
    "\n",
    "display(grouped, len(grouped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_download = grouped['unclustered_list'].tolist()\n",
    "\n",
    "print(to_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_check(ids, output_dir, replacements):\n",
    "    replacement_found = False  # Flag to track if a replacement is found\n",
    "\n",
    "    for id in ids:\n",
    "        url = f\"https://alphafold.ebi.ac.uk/files/AF-{id}-F1-model_v4.pdb\"\n",
    "        pdb_file_path = os.path.join(output_dir, f\"AF-{id}-F1-model_v4.pdb\")\n",
    "\n",
    "        # Download the file\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(pdb_file_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "\n",
    "            # Check if \"NoSuchKey\" is in the file\n",
    "            with open(pdb_file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                if \"NoSuchKey\" in content:\n",
    "                    os.remove(pdb_file_path)  # Delete the file if the string is found\n",
    "                else:\n",
    "                    replacements.append(id)\n",
    "                    replacement_found = True\n",
    "                    break  # Exit the loop if the string is not found\n",
    "\n",
    "    if not replacement_found:\n",
    "        replacements.append('None')  # Append None if no replacement was found\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the output files\n",
    "output_dir = \"C:/Users/odesa/Desktop/PDB_test/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop over each list of IDs\n",
    "replacements = []\n",
    "for id_list in to_download:\n",
    "    download_and_check(id_list, output_dir, replacements)\n",
    "    # The function will stop processing the current list if \"NoSuchKey\" is not found in any file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped['unclustered_list'] = replacements\n",
    "print(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv('../../data/clustering/cluster_maps/dl_endopeptidases.tsv',sep='\\t')\n",
    "\n",
    "df = df.iloc[:, 1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(original, grouped, on='dl_endopeptidases-mmseqs_cluster', how='left')\n",
    "\n",
    "original['replacements'] = merged_df['unclustered_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original['dl_endopeptidases-mmseqs_cluster'] = np.where(original['replacements'].notna(), \n",
    "                                                        original['replacements'], original['dl_endopeptidases-mmseqs_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe based on the condition\n",
    "filtered_df = grouped[grouped['unclustered_list'] == 'None']\n",
    "\n",
    "# Get the values from the 'dl_endopeptidases-mmseqs_cluster' column\n",
    "values = filtered_df['dl_endopeptidases-mmseqs_cluster'].values\n",
    "\n",
    "# Create a text file and write the values to it\n",
    "with open('E:/PDBs/dl_endopeptidases_representatives/to_delete.txt', 'w') as file:\n",
    "    for value in values:\n",
    "        file.write(str(value) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to be made into a function and optimized. Shoul dimplement multiprocessing for\n",
    "# downloading the PDBs and checking if they are empty\n",
    "\n",
    "\n",
    "enzymes = ['dd_endopeptidase']\n",
    "\n",
    "for enzyme in enzymes:\n",
    "    \n",
    "    # read in the cluster map, drop the first column\n",
    "    df = pd.read_csv('../../data/clustering/cluster_maps/' + enzyme + '.tsv',sep='\\t')\n",
    "    df = df.iloc[:, 1:4]\n",
    "\n",
    "    # drop the rows where the foldseek cluster is not empty\n",
    "    no_foldseek = df[pd.isna(df[df.columns[2]])]\n",
    "\n",
    "    # group by the mmseqs cluster and create a list of the unclustered ids\n",
    "    grouped = no_foldseek.groupby(f'{enzyme}-mmseqs_cluster')[f'{enzyme}-unclustered'].apply(list).reset_index(name='unclustered_list')\n",
    "\n",
    "    # create a directory for the PDB files\n",
    "    to_download = grouped['unclustered_list'].tolist()\n",
    "    output_dir = f\"/Users/odesa/Desktop/PDB_test/{enzyme}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Loop over each list of IDs\n",
    "    replacements = []\n",
    "    for id_list in to_download:\n",
    "        # download new PDBs, check if empty, creating a list of replacements\n",
    "        download_and_check(id_list, output_dir, replacements)\n",
    "\n",
    "    # Replace the 'unclustered_list' column with the list of replacements\n",
    "    grouped['unclustered_list'] = replacements\n",
    "\n",
    "    # Merge the original dataframe with the new one on mmseqs cluster\n",
    "    merged_df = pd.merge(df, grouped, on=f'{enzyme}-mmseqs_cluster', how='left')\n",
    "\n",
    "    # Create a new column with the list of replacements\n",
    "    df['replacements'] = merged_df['unclustered_list']\n",
    "\n",
    "    # Replace the mmseqs cluster with the list of replacements if it is not empty\n",
    "    df[f'{enzyme}-mmseqs_cluster'] = np.where(df['replacements'].notna(), \n",
    "                                                df['replacements'], \n",
    "                                                df[f'{enzyme}-mmseqs_cluster'])\n",
    "\n",
    "    # Filter the dataframe based on the condition\n",
    "    filtered_df = grouped[grouped['unclustered_list'] == 'None']\n",
    "\n",
    "    # Get the values from the 'dl_endopeptidases-mmseqs_cluster' column\n",
    "    values = filtered_df[f'{enzyme}-mmseqs_cluster'].values \n",
    "\n",
    "    # Create a text file and write the values to it\n",
    "    with open(f'/Users/odesa/Desktop/PDB_test/{enzyme}/to_delete.txt', 'w') as file:\n",
    "        for value in values:\n",
    "            file.write(str(value) + '\\n')\n",
    "\n",
    "    df.to_csv(f'~/Desktop/PDB_test/{enzyme}_cluster_update.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for enzyme in enzymes:\n",
    "    df = pd.read_csv(f'/Users/odesa/Desktop/PDB_test/{enzyme}_cluster_update.tsv', sep='\\t')\n",
    "\n",
    "    # df[f'{enzyme}s-mmseqs_cluster'] = df.apply(lambda row: row[f'{enzyme}s-unclustered'] if pd.isna(row[f'{enzyme}s-mmseqs_cluster']) else row[f'{enzyme}s-mmseqs_cluster'], axis=1)\n",
    "    \n",
    "    # df.to_csv(f'C:/Users/odesa/Desktop/{enzyme}_cluster_update.tsv', sep='\\t', index=False)\n",
    "    \n",
    "    unique_values = df[f'{enzyme}-mmseqs_cluster'].unique().tolist()\n",
    "\n",
    "    downloaded = []\n",
    "    with open('/Volumes/PGH-Backup/dd_endopeptidase_clustering/dd_endopeptidases_representatives/all_files.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            downloaded.append(line.strip())\n",
    "\n",
    "    to_remove = [x for x in downloaded if x not in unique_values]\n",
    "    print(f'{enzyme}: {len(to_remove)} {to_remove}')\n",
    "\n",
    "    with open('/Volumes/PGH-Backup/dd_endopeptidase_clustering/dd_endopeptidases_representatives/to_remove.txt', 'w') as file:\n",
    "        for value in to_remove:\n",
    "            file.write(str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enzymes = ['dl_endopeptidase', 'ld_carboxypeptidase', \n",
    "               'ld_endopeptidase', 'amidase',\n",
    "               'dd_carboxypeptidase', 'diadenylate_cyclase',\n",
    "               'muramidase', 'glucosaminidase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldseek_path = '/media/oliver/PGH_Backup/clustering/new_foldseek/'\n",
    "mmseqs_path = '/media/oliver/PGH_Backup/clustering/maps/'\n",
    "\n",
    "for enzyme in enzymes:\n",
    "    df = pd.read_csv(f'{mmseqs_path}{enzyme}_cluster_update.tsv', sep='\\t')\n",
    "    # print(df.head())\n",
    "    df = df.iloc[:, 0:2]\n",
    "    # print(df.head())\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    \n",
    "    df.to_csv(f'{mmseqs_path}{enzyme}_cluster_no_foldseek.tsv', sep='\\t', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldseek_path = '/media/oliver/PGH_Backup/clustering/new_foldseek/'\n",
    "mmseqs_path = '/media/oliver/PGH_Backup/clustering/maps/'\n",
    "\n",
    "for enzyme in enzymes:\n",
    "    create_maps(f'{mmseqs_path}{enzyme}_cluster_no_foldseek.tsv', f'{foldseek_path}{enzyme}/foldseek_result_{enzyme}_cluster.tsv', f'{enzyme}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = []\n",
    "\n",
    "for enzyme in enzymes:\n",
    "    df = pd.read_csv(f'../../data/clustering/cluster_maps/{enzyme}.tsv', sep='\\t')\n",
    "    \n",
    "    print(f\"Original DataFrame shape for {enzyme}: {df.shape}\")\n",
    "\n",
    "    # Get the counts of each value in the target column\n",
    "    value_counts = df.iloc[:, 3].value_counts()\n",
    "\n",
    "    # Identify the values that occur only once\n",
    "    single_occurrence_values = value_counts[value_counts == 1].index\n",
    "\n",
    "    # Drop rows where the column value is one of those that occur only once\n",
    "    df = df[~df.iloc[:, 3].isin(single_occurrence_values)]\n",
    "\n",
    "    df.to_csv(f'../../data/clustering/cluster_maps/{enzyme}.tsv', sep='\\t', index=False)\n",
    "\n",
    "    print(f\"Modified DataFrame shape for {enzyme}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_maps('/Volumes/PGH-Backup/foldseek/dd_endopeptidases_clusters.tsv', '/Volumes/PGH-Backup/foldseek/foldseek_result/foldseek_result_cluster.tsv', 'dd_endopeptidase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the TSV files into DataFrames\n",
    "df_10_columns = pd.read_csv('../../data/clustering/cluster_maps/combined_clusters.tsv', sep='\\t')\n",
    "df_3_columns = pd.read_csv('../../data/clustering/cluster_maps/dd_endopeptidase.tsv', sep='\\t')\n",
    "\n",
    "# Concatenate the DataFrames along the columns\n",
    "combined_df = pd.concat([df_10_columns, df_3_columns], axis=1)\n",
    "\n",
    "# print(combined_df.columns)\n",
    "\n",
    "# drop the column called Unnamed: 0\n",
    "\n",
    "combined_df = combined_df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# print(combined_df.columns)\n",
    "\n",
    "# Save the combined DataFrame to a TSV file\n",
    "combined_df.to_csv('../../data/clustering/cluster_maps/combined_clusters.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_endopeptidase = pd.read_csv('../../data/clustering/cluster_maps/dd_endopeptidase.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_endopeptidase_cluster = pd.read_csv('/Users/odesa/Desktop/PDB_test/dd_endopeptidase_cluster_update.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "display(dd_endopeptidase_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "dd_endopeptidase_cluster.loc[dd_endopeptidase_cluster['dd_endopeptidase-foldseek_cluster'].isna(), 'dd_endopeptidase-foldseek_cluster'] = dd_endopeptidase_cluster['replacements']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "display(dd_endopeptidase_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "dd_endopeptidase_cluster.drop(columns=['replacements'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "dd_endopeptidase_cluster.to_csv('../../data/clustering/cluster_maps/dd_endopeptidase.tsv', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "master_map = pd.read_csv('../../data/clustering/cluster_maps/combined_clusters.tsv', sep = '\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "master_map.drop(columns=['dd_endopeptidase-unclustered', 'dd_endopeptidase-mmseqs_cluster', 'dd_endopeptidase-foldseek_cluster'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "master_map = pd.concat([master_map, dd_endopeptidase_cluster], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "master_map.to_csv('../../data/clustering/cluster_maps/combined_clusters.tsv', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing for Clustering Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def cluster_humann_table_with_detailed_checks(humann_feather, cluster_tsv):\n",
    "    \"\"\"Cluster the humann table for each of the PGH enzymes with detailed checks.\"\"\"\n",
    "    \n",
    "    # read in the humann table\n",
    "    humann_df = pd.read_feather(humann_feather)\n",
    "\n",
    "    # read in the clustering dataframes\n",
    "    cluster_df = pd.read_csv(cluster_tsv, sep='\\t', low_memory=False)\n",
    "\n",
    "    # list of enzymes\n",
    "    enzymes = ['DL-endopeptidase', 'LD-carboxypeptidase', \n",
    "               'LD-endopeptidase', 'Glucosaminidase',\n",
    "               'DD-carboxypeptidase', 'DD-endopeptidase',\n",
    "               'Amidase', 'Muramidase']\n",
    "\n",
    "    clustered_df = pd.DataFrame()\n",
    "\n",
    "    # Create a mapping for each enzyme's clusters beforehand\n",
    "    cluster_map = {}\n",
    "    for enzyme in enzymes:\n",
    "        enzyme_col = f\"{enzyme.replace('-', '_').lower()}-unclustered\"\n",
    "        cluster_col = f\"{enzyme.replace('-', '_').lower()}-foldseek_cluster\"\n",
    "        enzyme_cluster_map = cluster_df.set_index(enzyme_col)[cluster_col].to_dict()\n",
    "        cluster_map[enzyme] = enzyme_cluster_map\n",
    "\n",
    "    # Process each enzyme\n",
    "    for enzyme in enzymes:\n",
    "        df = humann_df.loc[:, humann_df.columns.str.startswith(enzyme)]\n",
    "        column_names = df.columns.tolist()\n",
    "\n",
    "        print(f'{len(column_names)} {enzyme} found')\n",
    "\n",
    "        # Extract the UniRef IDs from the column names\n",
    "        column_ids = [x.split('_')[2] for x in column_names]\n",
    "\n",
    "        # Get the foldseek cluster for each UniRef ID\n",
    "        results = []\n",
    "        unclustered = []\n",
    "        clusters_info = {}\n",
    "        for id in column_ids:\n",
    "            result = cluster_map[enzyme].get(id, \"unclustered\")\n",
    "            if result == \"unclustered\":\n",
    "                unclustered.append(id)\n",
    "            else:\n",
    "                cluster_id = f\"{enzyme}-{result}\"\n",
    "                clusters_info.setdefault(cluster_id, []).append(id)\n",
    "            results.append(f\"{enzyme}-{result}\" if result != \"unclustered\" else \"unclustered\")\n",
    "        \n",
    "        print(f\"{len(results)} {enzyme} processed, {len(unclustered)} {enzyme} unclustered\")\n",
    "\n",
    "        # Detailed inspection of clusters\n",
    "        for cluster_id, ids in clusters_info.items():\n",
    "            print(f\"Cluster {cluster_id} contains {len(ids)} UniRef100 IDs: {ids[:5]}...\")  # Print first 5 IDs for brevity\n",
    "\n",
    "        # Print the unclustered IDs\n",
    "        if unclustered:\n",
    "            print(f\"Unclustered UniRef100 IDs for {enzyme}: {unclustered[:5]}...\")  # Print first 5 unclustered IDs for brevity\n",
    "\n",
    "        # Replace the column names with the foldseek cluster\n",
    "        df.columns = results\n",
    "\n",
    "        # Aggregate the columns by foldseek cluster\n",
    "        agg_df = df.T.groupby(df.columns).sum().T\n",
    "\n",
    "        # Add the aggregated df to the clustered df\n",
    "        clustered_df = pd.concat([clustered_df, agg_df], axis=1)\n",
    "    \n",
    "    # Add the sample id column back to the dataframe\n",
    "    clustered_df['sample_id'] = humann_df['sample_id']\n",
    "\n",
    "    return clustered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "new_clust = cluster_humann_table_with_detailed_checks(\"E:\\\\CRC\\\\PRJEB7774\\\\humann\\\\new_combined\\\\clean_joined_genefamilies_relab_7774.feather\", \"../../data/clustering/cluster_maps/combined_clusters.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155, 1550)\n"
     ]
    }
   ],
   "source": [
    "print(new_clust.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def validate_enzyme_clusters(cluster_tsv_path):\n",
    "    \"\"\"Validate that each UniRef100 ID maps to only one unique Foldseek cluster within each enzyme type.\"\"\"\n",
    "    \n",
    "    # Read in the clustering dataframe\n",
    "    cluster_df = pd.read_csv(cluster_tsv_path, sep='\\t', low_memory=False)\n",
    "    \n",
    "    # Define the enzymes and their corresponding column groups\n",
    "    enzymes = ['dl_endopeptidase', 'ld_carboxypeptidase', 'ld_endopeptidase', 'glucosaminidase',\n",
    "               'diadenylate_cyclase', 'muramidase', 'dd_carboxypeptidase', 'amidase', 'dd_endopeptidase']\n",
    "    \n",
    "    # For each enzyme, check if UniRef100 IDs map to more than one Foldseek cluster\n",
    "    validation_results = {}\n",
    "    for enzyme in enzymes:\n",
    "        # Extract columns related to the current enzyme\n",
    "        unclustered_col = f'{enzyme}-unclustered'\n",
    "        foldseek_col = f'{enzyme}-foldseek_cluster'\n",
    "        \n",
    "        # Group by UniRef100 IDs and check how many unique Foldseek clusters they map to\n",
    "        duplicate_check = cluster_df.groupby(unclustered_col).agg({foldseek_col: pd.Series.nunique})\n",
    "        \n",
    "        # Identify cases where a UniRef100 ID maps to multiple Foldseek clusters\n",
    "        duplicates = duplicate_check[duplicate_check[foldseek_col] > 1]\n",
    "        \n",
    "        # Store the result\n",
    "        if len(duplicates) > 0:\n",
    "            validation_results[enzyme] = duplicates\n",
    "            print(f\"Warning: {len(duplicates)} UniRef100 IDs for {enzyme} map to multiple clusters.\")\n",
    "        else:\n",
    "            print(f\"Validation passed for {enzyme}: All UniRef100 IDs map to a single Foldseek cluster.\")\n",
    "    \n",
    "    return validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation passed for dl_endopeptidase: All UniRef100 IDs map to a single Foldseek cluster.\n",
      "Validation passed for ld_carboxypeptidase: All UniRef100 IDs map to a single Foldseek cluster.\n",
      "Validation passed for ld_endopeptidase: All UniRef100 IDs map to a single Foldseek cluster.\n",
      "Validation passed for glucosaminidase: All UniRef100 IDs map to a single Foldseek cluster.\n",
      "Validation passed for diadenylate_cyclase: All UniRef100 IDs map to a single Foldseek cluster.\n",
      "Validation passed for muramidase: All UniRef100 IDs map to a single Foldseek cluster.\n",
      "Validation passed for dd_carboxypeptidase: All UniRef100 IDs map to a single Foldseek cluster.\n",
      "Validation passed for amidase: All UniRef100 IDs map to a single Foldseek cluster.\n",
      "Validation passed for dd_endopeptidase: All UniRef100 IDs map to a single Foldseek cluster.\n"
     ]
    }
   ],
   "source": [
    "res = validate_enzyme_clusters(\"../../data/clustering/cluster_maps/combined_clusters.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_humann_table_improved(humann_feather, cluster_tsv):\n",
    "    \"\"\"Cluster the humann table for each of the PGH enzymes and store cluster information.\"\"\"\n",
    "    \n",
    "    # read in the humann table\n",
    "    humann_df = pd.read_feather(humann_feather)\n",
    "\n",
    "    # read in the clustering dataframes\n",
    "    cluster_df = pd.read_csv(cluster_tsv, sep='\\t', low_memory=False)\n",
    "\n",
    "    # list of enzymes\n",
    "    enzymes = ['DL-endopeptidase', 'LD-carboxypeptidase', \n",
    "               'LD-endopeptidase', 'Glucosaminidase',\n",
    "               'DD-carboxypeptidase', 'DD-endopeptidase',\n",
    "               'Amidase', 'Muramidase']\n",
    "    \n",
    "    extra_classes = ['Saga', 'UC118']\n",
    "\n",
    "    clustered_df = pd.DataFrame()\n",
    "    \n",
    "    clustered_df = pd.DataFrame()\n",
    "    \n",
    "    # This will store information about each cluster\n",
    "    cluster_info_list = []\n",
    "\n",
    "    # Create a mapping for each enzyme's clusters beforehand\n",
    "    cluster_map = {}\n",
    "    for enzyme in enzymes:\n",
    "        enzyme_col = f\"{enzyme.replace('-', '_').lower()}-unclustered\"\n",
    "        cluster_col = f\"{enzyme.replace('-', '_').lower()}-foldseek_cluster\"\n",
    "        enzyme_cluster_map = cluster_df.set_index(enzyme_col)[cluster_col].to_dict()\n",
    "        cluster_map[enzyme] = enzyme_cluster_map\n",
    "\n",
    "    # Process each enzyme\n",
    "    for enzyme in enzymes:\n",
    "        df = humann_df.loc[:, humann_df.columns.str.startswith(enzyme)]\n",
    "        column_names = df.columns.tolist()\n",
    "\n",
    "        print(f'{len(column_names)} {enzyme} found')\n",
    "\n",
    "        # Extract the UniRef IDs from the column names\n",
    "        column_ids = [x.split('_')[2] for x in column_names]\n",
    "\n",
    "        # Get the foldseek cluster for each UniRef ID\n",
    "        results = []\n",
    "        clusters_info = {}\n",
    "        for id in column_ids:\n",
    "            result = cluster_map[enzyme].get(id, \"unclustered\")\n",
    "            if result != \"unclustered\":\n",
    "                cluster_id = f\"{enzyme}-{result}\"\n",
    "                clusters_info.setdefault(cluster_id, []).append(id)\n",
    "            results.append(f\"{enzyme}-{result}\" if result != \"unclustered\" else \"unclustered\")\n",
    "        \n",
    "        # Replace the column names with the foldseek cluster\n",
    "        df.columns = results\n",
    "\n",
    "        # Aggregate the columns by foldseek cluster\n",
    "        agg_df = df.T.groupby(df.columns).sum().T\n",
    "\n",
    "        # Add the aggregated df to the clustered df\n",
    "        clustered_df = pd.concat([clustered_df, agg_df], axis=1)\n",
    "\n",
    "        # Collect the cluster information for analysis\n",
    "        for cluster_id, ids in clusters_info.items():\n",
    "            # Sum the final abundance for this cluster\n",
    "            final_abundance = agg_df[cluster_id].sum()\n",
    "\n",
    "            # Add the cluster info\n",
    "            cluster_info_list.append({\n",
    "                'cluster_id': cluster_id,\n",
    "                'enzyme': enzyme,\n",
    "                'num_uniref_ids': len(ids),\n",
    "                'final_abundance': final_abundance\n",
    "            })\n",
    "    \n",
    "    # Aggregate the extra classes (Saga and uc118) into single columns each\n",
    "    for extra_class in extra_classes:\n",
    "        df_extra = humann_df.loc[:, humann_df.columns.str.startswith(extra_class)]\n",
    "        \n",
    "        if not df_extra.empty:\n",
    "            print(f'{len(df_extra.columns)} {extra_class} found')\n",
    "            # Sum all columns for the extra class into one column\n",
    "            extra_class_agg = df_extra.sum(axis=1)\n",
    "            clustered_df[f'{extra_class}_aggregated'] = extra_class_agg\n",
    "\n",
    "            # Collect the info for the extra classes\n",
    "            cluster_info_list.append({\n",
    "                'cluster_id': f'{extra_class}_aggregated',\n",
    "                'enzyme': extra_class,\n",
    "                'num_uniref_ids': df_extra.shape[1],\n",
    "                'final_abundance': extra_class_agg.sum()\n",
    "            })\n",
    "        else:\n",
    "            print(f'No {extra_class} found')\n",
    "\n",
    "    # Add the sample id column back to the dataframe\n",
    "    clustered_df['sample_id'] = humann_df['sample_id']\n",
    "    \n",
    "    # Convert cluster info list to DataFrame\n",
    "    cluster_info_df = pd.DataFrame(cluster_info_list)\n",
    "    \n",
    "    return clustered_df, cluster_info_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369 DL-endopeptidase found\n",
      "4154 LD-carboxypeptidase found\n",
      "37 LD-endopeptidase found\n",
      "2602 Glucosaminidase found\n",
      "11868 DD-carboxypeptidase found\n",
      "2659 DD-endopeptidase found\n",
      "21146 Amidase found\n",
      "34888 Muramidase found\n",
      "No Saga found\n",
      "99 UC118 found\n"
     ]
    }
   ],
   "source": [
    "new_clust, new_clust_info = cluster_humann_table_improved(\"E:\\\\CRC\\\\PRJEB7774\\\\humann\\\\new_combined\\\\clean_joined_genefamilies_relab_7774.feather\", \"../../data/clustering/cluster_maps/combined_clusters.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\odesa\\Desktop\\Code\\CRC-Final\\src\\clustering\\functions.py:314: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
      "  def group_humann_table(humann_table):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original width: 80749, Grouped width: 12\n"
     ]
    }
   ],
   "source": [
    "group = group_humann_table(\"E:\\\\CRC\\\\PRJEB7774\\\\humann\\\\new_combined\\\\clean_joined_genefamilies_relab_7774.feather\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
