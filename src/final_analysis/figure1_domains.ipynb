{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from matplotlib.colors import Normalize, ListedColormap\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first load tables\n",
    "\n",
    "domain_table = pd.read_csv('/Volumes/PGH-Backup/domains/IPS/all_IPS_results.tsv', sep='\\t', header=None)\n",
    "\n",
    "display(domain_table, domain_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the enzyme name and aggregate the domain column as a list\n",
    "grouped_df = domain_table.groupby(0)[5].apply(list).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df_clean = grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df_clean[0] = grouped_df_clean[0].str.split('|').str[0]\n",
    "\n",
    "display(grouped_df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df_clean.to_csv('/Volumes/PGH-Backup/domains/IPS/all_IPS_results_grouped.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Merging with cluster table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Temporary swap to a windows enviorment, paths changed accordingly\n",
    "# grouped_df_clean = pd.read_csv(\"E:\\\\domains\\\\IPS\\\\all_IPS_results_grouped.tsv\", sep='\\t', header=None)\n",
    "grouped_df_clean = pd.read_csv(\"/Volumes/PGH-Backup/domains/IPS/all_IPS_results_grouped.tsv\", sep='\\t', header=None)\n",
    "\n",
    "display(grouped_df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_domain_table(df):\n",
    "    \"\"\"\n",
    "    Function to format the domain table for future use\n",
    "    \"\"\"\n",
    "    # Rename columns\n",
    "    df.rename(columns={0: 'FullIdentifier', 1: 'Domains'}, inplace=True)\n",
    "    \n",
    "    # Split the FullIdentifier column into separate columns\n",
    "    df['Uniref'] = df['FullIdentifier'].str.split('_').str[2]\n",
    "\n",
    "    df['Enzyme'] = df['FullIdentifier'].str.split('_').str[0]\n",
    "\n",
    "    df.drop_duplicates(subset='Uniref', inplace=True)\n",
    "\n",
    "    enzymes = df['Enzyme'].unique()\n",
    "\n",
    "    return df, enzymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in cluster map\n",
    "# cluster_map = pd.read_csv(\"E:\\\\clustering\\\\newest_cluster_maps\\\\catted_maps.tsv\", sep='\\t', index_col=0, header=None)\n",
    "cluster_map = pd.read_csv(\"/Volumes/PGH-Backup/clustering/newest_cluster_maps/catted_maps.tsv\", sep='\\t', index_col=0, header=None)\n",
    "\n",
    "display(cluster_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_cluster_map(df):\n",
    "    \"\"\"\n",
    "    Function to format the cluster map for future use, return list of unique enzymes\n",
    "    \"\"\"\n",
    "    # Rename columns\n",
    "    df.rename(columns={1: 'unclustered', 2: 'mmseqs', 3: 'foldseek'}, inplace=True)\n",
    "\n",
    "    df.drop_duplicates(subset='unclustered', inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_convert(domain_string):\n",
    "    domain_string = [ast.literal_eval(x) for x in domain_string]\n",
    "    \n",
    "    domain_string = [\n",
    "        item\n",
    "        for sublist in domain_string\n",
    "        for item in sublist\n",
    "    ]\n",
    "    \n",
    "    return domain_string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset by enzymes and merge with cluster map\n",
    "\n",
    "for enzyme in enzymes:\n",
    "    grouped_df_clean_subset = grouped_df_clean[grouped_df_clean['Enzyme'] == enzyme]\n",
    "    merged_df = pd.merge(grouped_df_clean_subset, cluster_map, left_on='Uniref', right_on='unclustered', how='left')\n",
    "    merged_df = merged_df.dropna(subset=['foldseek'])\n",
    "\n",
    "    if merged_df.shape[0] > 0:\n",
    "        grouped_merged_domain_cluster = merged_df.groupby('foldseek').agg({\n",
    "                'Domains': list,    # Aggregate Domains into a list\n",
    "                'Uniref': list,     # Aggregate Uniref into a list\n",
    "                'Enzyme': set       # Aggregate Enzyme into a set (to remove duplicates)\n",
    "            }).reset_index()\n",
    "        \n",
    "        grouped_merged_domain_cluster['member_count'] = grouped_merged_domain_cluster['Uniref'].apply(len)\n",
    "\n",
    "        grouped_merged_domain_cluster['Domains'] = grouped_merged_domain_cluster['Domains'].apply(\n",
    "            lambda x: clean_and_convert(x) if isinstance(x, list) else x)\n",
    "        \n",
    "        print(grouped_merged_domain_cluster.head())\n",
    "\n",
    "        grouped_merged_domain_cluster.to_csv(f\"/Volumes/PGH-Backup/domains/IPS/{enzyme}_IPS_results_grouped.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Network Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_domain_similarity(subset_df):\n",
    "\n",
    "    subset_df = pd.read_csv(subset_df, sep='\\t')\n",
    "    \n",
    "    # Initialize to track how many proteins have each domain in each cluster\n",
    "    domain_occurrences = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # Iterate over the dataframe to count domain presence per protein\n",
    "    for index, row in subset_df.iterrows():\n",
    "        cluster_id = row['foldseek']\n",
    "        domains = row['Domains']\n",
    "\n",
    "        # Check if 'domains' is a valid non-empty list or string\n",
    "        if isinstance(domains, str) and domains.strip() != '':\n",
    "            try:\n",
    "                # Safely evaluate the string into a list\n",
    "                unique_domains = set(eval(domains))\n",
    "            except:\n",
    "                # Skip any domains that can't be evaluated\n",
    "                continue\n",
    "        elif isinstance(domains, (list, tuple, np.ndarray)) and len(domains) > 0:\n",
    "            unique_domains = set(domains)\n",
    "        else:\n",
    "            # Skip if 'domains' is None, NaN, empty, or not a valid type\n",
    "            continue\n",
    "\n",
    "        # Count each domain in the set\n",
    "        for domain in unique_domains:\n",
    "            domain_occurrences[cluster_id][domain] += 1\n",
    "\n",
    "    # Calculate the percentage of proteins with each domain in each cluster\n",
    "    domain_percentages = {}\n",
    "    for cluster, domains_dict in domain_occurrences.items():\n",
    "        member_count = subset_df.loc[subset_df['foldseek'] == cluster, 'member_count'].values[0]\n",
    "        domain_percentages[cluster] = {domain: count / member_count for domain, count in domains_dict.items()}\n",
    "\n",
    "    # Convert domain percentages to a matrix for similarity calculation\n",
    "    # Create a list of all unique domains across all clusters\n",
    "    all_domains = set(domain for cluster_domains in domain_percentages.values() for domain in cluster_domains.keys())\n",
    "\n",
    "    # Create a matrix of domain percentages for each cluster\n",
    "    cluster_ids = list(domain_percentages.keys())\n",
    "    domain_matrix = np.zeros((len(cluster_ids), len(all_domains)))\n",
    "\n",
    "    # Mapping of cluster IDs and domain indices to facilitate matrix population\n",
    "    cluster_idx_map = {cluster_id: idx for idx, cluster_id in enumerate(cluster_ids)}\n",
    "    domain_idx_map = {domain: idx for idx, domain in enumerate(all_domains)}\n",
    "\n",
    "    # Populate the matrix with domain percentages\n",
    "    for cluster_id, domains_dict in domain_percentages.items():\n",
    "        for domain, percentage in domains_dict.items():\n",
    "            cluster_idx = cluster_idx_map[cluster_id]\n",
    "            domain_idx = domain_idx_map[domain]\n",
    "            domain_matrix[cluster_idx, domain_idx] = percentage\n",
    "\n",
    "    # Calculate cosine similarity between clusters based on domain matrix\n",
    "    cosine_sim = cosine_similarity(domain_matrix)\n",
    "\n",
    "    # Convert the similarity matrix to edge list for significant similarities\n",
    "    # We consider a similarity significant if it's above 0.1 (can adjust)\n",
    "    significant_similarity_threshold = 0.8\n",
    "    significant_edges = []\n",
    "    for i in range(len(cluster_ids)):\n",
    "        for j in range(i+1, len(cluster_ids)):\n",
    "            if cosine_sim[i, j] > significant_similarity_threshold:\n",
    "                significant_edges.append((cluster_ids[i], cluster_ids[j], cosine_sim[i, j]))\n",
    "        \n",
    "    return significant_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network(enzyme_type, subset_df, significant_edges, association_table=None, title=None, global_min_size=None, global_max_size=None):\n",
    "    \"\"\" Plot the network graph for each unique enzyme type. Adjust node colors based on\n",
    "        the enzyme type and node sizes based on cluster sizes. \"\"\"\n",
    "    \n",
    "    subset_df = pd.read_csv(subset_df, sep='\\t')\n",
    "\n",
    "    # Create the network graph\n",
    "    G_adjusted_similarity = nx.Graph()\n",
    "\n",
    "    # Subset cluster_ids and edges based on the enzyme type in subset_df\n",
    "    cluster_ids_subset = subset_df['foldseek'].tolist()\n",
    "    edges_subset = [(u, v, w) for u, v, w in significant_edges if u in cluster_ids_subset and v in cluster_ids_subset]\n",
    "\n",
    "    # Add nodes (clusters) for the subset\n",
    "    G_adjusted_similarity.add_nodes_from(cluster_ids_subset)\n",
    "\n",
    "    # Add edges with weights based on cosine similarity for the subset\n",
    "    G_adjusted_similarity.add_weighted_edges_from(edges_subset)\n",
    "\n",
    "    # --- NEW: Get the connected components (subclusters) ---\n",
    "    connected_components = list(nx.connected_components(G_adjusted_similarity))\n",
    "\n",
    "    # Rank the connected components by their size (number of nodes)\n",
    "    connected_components_sorted = sorted(connected_components, key=len, reverse=True)\n",
    "\n",
    "    # Keep the top 5 largest clusters, color them, and set the rest to grey\n",
    "    top_n = 5\n",
    "    cmap = plt.get_cmap('tab10')  # Use a colormap with 10 distinct colors\n",
    "    cluster_colors = {i: cmap(i / top_n) for i in range(top_n)}  # Assign colors to top 5 clusters\n",
    "    grey_color = 'grey'\n",
    "\n",
    "    # Create a mapping of node to its subcluster color\n",
    "    node_color_map = {}\n",
    "    for i, component in enumerate(connected_components_sorted):\n",
    "        if i < top_n:\n",
    "            # Assign a color from the colormap to top 5 clusters\n",
    "            for node in component:\n",
    "                node_color_map[node] = cluster_colors[i]\n",
    "        else:\n",
    "            # Assign grey color to the remaining smaller clusters\n",
    "            for node in component:\n",
    "                node_color_map[node] = grey_color\n",
    "\n",
    "    # Get the cluster sizes from the 'member_count' column in the subset_df\n",
    "    cluster_sizes = subset_df.set_index('foldseek')['member_count'].to_dict()\n",
    "\n",
    "    # Normalize cluster sizes globally\n",
    "    if global_min_size is None:\n",
    "        global_min_size = min(cluster_sizes.values())\n",
    "    if global_max_size is None:\n",
    "        global_max_size = max(cluster_sizes.values())\n",
    "\n",
    "    min_size = 20\n",
    "    max_size = 1000\n",
    "    node_sizes = [\n",
    "        ((cluster_sizes[node] - global_min_size) / (global_max_size - global_min_size) * (max_size - min_size) + min_size)\n",
    "        if node in cluster_sizes else min_size\n",
    "        for node in G_adjusted_similarity.nodes()\n",
    "    ]\n",
    "\n",
    "    # --- NEW: Extract node colors based on cluster assignment ---\n",
    "    node_colors = [node_color_map[node] for node in G_adjusted_similarity.nodes()]\n",
    "\n",
    "    # Visualize the adjusted network for the enzyme type\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    pos = nx.spring_layout(G_adjusted_similarity, seed=42, k=0.5)  # Adjust the 'k' parameter to control node spacing\n",
    "\n",
    "    # Draw the network\n",
    "    nodes = nx.draw_networkx_nodes(G_adjusted_similarity, pos, node_color=node_colors, node_size=node_sizes, alpha=0.8)\n",
    "    nx.draw_networkx_edges(G_adjusted_similarity, pos, alpha=0.5)\n",
    "    nx.draw_networkx_labels(G_adjusted_similarity, pos, font_size=5, alpha=0.7)\n",
    "\n",
    "    # --- NEW: Add title and legend ---\n",
    "    if title is not None:\n",
    "        plt.title(f\"{title} - {enzyme_type}\")\n",
    "    else:\n",
    "        plt.title(f\"Adjusted Network Graph for {enzyme_type}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # --- NEW: Add legend showing only top 5 clusters ---\n",
    "    handles = [\n",
    "        plt.Line2D([0], [0], marker='o', color='w', label=f'Cluster {i+1}', \n",
    "                   markerfacecolor=cluster_colors[i], markersize=10)\n",
    "        for i in range(top_n)\n",
    "    ]\n",
    "    handles.append(plt.Line2D([0], [0], marker='o', color='w', label=f'Other Clusters', \n",
    "                   markerfacecolor=grey_color, markersize=10))\n",
    "    \n",
    "    plt.legend(handles=handles, title='Subclusters', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Save the plot for each enzyme type\n",
    "    plt.savefig(f'./{enzyme_type}_adjusted_network.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "    return G_adjusted_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_domains(subset_df, G_adjusted_similarity, top_n=5, top_domains=3):\n",
    "    subset_df = pd.read_csv(subset_df, sep='\\t')\n",
    "    \n",
    "    # Get the connected components (clusters)\n",
    "    connected_components = list(nx.connected_components(G_adjusted_similarity))\n",
    "    \n",
    "    # Sort the connected components by size (number of nodes) in descending order\n",
    "    connected_components_sorted = sorted(connected_components, key=len, reverse=True)\n",
    "    \n",
    "    # Limit to the top_n largest clusters\n",
    "    top_connected_components = connected_components_sorted[:top_n]\n",
    "\n",
    "    # Create a dictionary to store domains for each of the top_n clusters\n",
    "    cluster_domains = {}\n",
    "\n",
    "    for i, component in enumerate(top_connected_components):\n",
    "        cluster_name = f'Cluster_{i+1}'\n",
    "        domain_counter = Counter()  # Use a Counter to track domain frequencies\n",
    "        for node in component:\n",
    "            domains_str = subset_df.loc[subset_df['foldseek'] == node, 'Domains'].values[0]\n",
    "            try:\n",
    "                domains = eval(domains_str)  # Safely evaluate domain strings\n",
    "                domain_counter.update(domains)  # Count domain occurrences\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # Get the top N most common domains in the cluster\n",
    "        cluster_domains[cluster_name] = domain_counter.most_common(top_domains)\n",
    "\n",
    "    # Print and return the top 3 domains for each cluster\n",
    "    for cluster, top_domains in cluster_domains.items():\n",
    "        print(f\"Top {len(top_domains)} domains in {cluster}: {top_domains}\")\n",
    "\n",
    "    return cluster_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enzymes = ['Amidase', 'DD-carboxypeptidase', 'DD-endopeptidase', 'DL-endopeptidase', \n",
    "           'Glucosaminidase', 'LD-carboxypeptidase', 'Muramidase']\n",
    "\n",
    "path = f'/Volumes/PGH-Backup/domains/IPS/clustered/{enzyme}_IPS_results_grouped.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this part of the code to pass in global min/max sizes across all enzymes\n",
    "global_min_size = float('inf')\n",
    "global_max_size = float('-inf')\n",
    "\n",
    "# Loop through each enzyme type and calculate global min/max cluster sizes first\n",
    "for enzyme in enzymes:\n",
    "    path = f'/Volumes/PGH-Backup/domains/IPS/clustered/{enzyme}_IPS_results_grouped.tsv'\n",
    "    \n",
    "    subset_df = pd.read_csv(path, sep='\\t')\n",
    "    cluster_sizes = subset_df['member_count'].values\n",
    "\n",
    "    global_min_size = min(global_min_size, min(cluster_sizes))\n",
    "    global_max_size = max(global_max_size, max(cluster_sizes))\n",
    "\n",
    "# Now, loop again to plot each enzyme network with consistent global node size scaling\n",
    "for enzyme in enzymes:\n",
    "    path = f'/Volumes/PGH-Backup/domains/IPS/clustered/{enzyme}_IPS_results_grouped.tsv'\n",
    "    \n",
    "    significant_edges = calculate_domain_similarity(path)\n",
    "\n",
    "    G_adjusted_similarity = plot_network(\n",
    "        enzyme_type=enzyme,\n",
    "        subset_df=path,\n",
    "        significant_edges=significant_edges,\n",
    "        title=None,\n",
    "        global_min_size=global_min_size,\n",
    "        global_max_size=global_max_size\n",
    "    )\n",
    "    \n",
    "    print(f\"Clusters for {enzyme}\")\n",
    "    cluster_domains = extract_top_domains(path, G_adjusted_similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Comparison with clustering methods, how different are domain inclusion stats between sequence clustering and structural?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: How do domain percentages change from sequence clustering to foldseek clustering?**\n",
    "1. merge domain table with cluster maps\n",
    "2. groupby mmseqs cluster reps\n",
    "3. clean formatting for domain lists\n",
    "4. calculate stats on % domain inclusion, do same for foldseek clusters\n",
    "5. visualizations to compare "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: merge domain table w cluster maps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_df_clean = pd.read_csv(\"E:/domains/IPS/clustered/all_IPS_results_grouped.tsv\", sep='\\t', header=None)\n",
    "\n",
    "# mac\n",
    "grouped_df_clean = pd.read_csv(\"/Volumes/PGH-Backup/domains/IPS/clustered/all_IPS_results_grouped.tsv\", sep='\\t', header=None)\n",
    "\n",
    "display(grouped_df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grouped_df_clean, enzymes = format_domain_table(grouped_df_clean)\n",
    "\n",
    "display(new_grouped_df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_map = pd.read_csv(\"E:/clustering/newest_cluster_maps/catted_maps.tsv\", sep='\\t', index_col=0, header=None)\n",
    "\n",
    "# mac\n",
    "cluster_map = pd.read_csv(\"/Volumes/PGH-Backup/clustering/newest_cluster_maps/catted_maps.tsv\", sep='\\t', index_col=0, header=None)\n",
    "\n",
    "clean_cluster_map = format_cluster_map(cluster_map)\n",
    "\n",
    "display(clean_cluster_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop foldseek column\n",
    "clean_cluster_map.drop(columns=['foldseek'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enzymes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps 2 & 3: group by mmseqs & format domain info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for enzyme in enzymes:\n",
    "    grouped_df_clean_subset = new_grouped_df_clean[new_grouped_df_clean['Enzyme'] == enzyme]\n",
    "    merged_df = pd.merge(grouped_df_clean_subset, cluster_map, left_on='Uniref', right_on='unclustered', how='left')\n",
    "    merged_df = merged_df.dropna(subset=['mmseqs'])\n",
    "\n",
    "    if merged_df.shape[0] > 0:\n",
    "        grouped_merged_domain_cluster = merged_df.groupby('mmseqs').agg({\n",
    "                'Domains': list,    # Aggregate Domains into a list\n",
    "                'Uniref': list,     # Aggregate Uniref into a list\n",
    "                'Enzyme': set       # Aggregate Enzyme into a set (to remove duplicates)\n",
    "            }).reset_index()\n",
    "        \n",
    "        grouped_merged_domain_cluster['member_count'] = grouped_merged_domain_cluster['Uniref'].apply(len)\n",
    "\n",
    "        grouped_merged_domain_cluster['Domains'] = grouped_merged_domain_cluster['Domains'].apply(\n",
    "            lambda x: clean_and_convert(x) if isinstance(x, list) else x)\n",
    "        \n",
    "        print(grouped_merged_domain_cluster.head())\n",
    "\n",
    "        grouped_merged_domain_cluster.to_csv(f\"/Volumes/PGH-Backup/domains/IPS/{enzyme}_IPS_results_grouped_mmseqs.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Calculate stats for domain inclusion for mmseqs clusters compared to foldseek**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_view = pd.read_csv(\"/Volumes/PGH-Backup/domains/IPS/mmseqs_groups/Amidase_IPS_results_grouped_mmseqs.tsv\", sep='\\t')\n",
    "\n",
    "display(test_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_view['Domains'] = test_view['Domains'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Now proceed with your logic\n",
    "for idx, row in test_view.iterrows():\n",
    "    unique_domains = set(row['Domains'])  # Get unique domains from each row\n",
    "\n",
    "    # For each unique domain in the row\n",
    "    for domain in unique_domains:\n",
    "        if domain not in test_view.columns:  # If the domain is not already a column\n",
    "            test_view[domain] = 0.0  # Initialize the column with 0\n",
    "\n",
    "        # Fill the column with the proportion of the domain in the current row\n",
    "        test_view.at[idx, domain] = row['Domains'].count(domain) / row['member_count']\n",
    "\n",
    "# Display the updated dataframe\n",
    "display(test_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
